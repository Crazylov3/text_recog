# change to list chars of your dataset or use default vietnamese chars
vocab: 'aAàÀảẢãÃáÁạẠăĂằẰẳẲẵẴắẮặẶâÂầẦẩẨẫẪấẤậẬbBcCdDđĐeEèÈẻẺẽẼéÉẹẸêÊềỀểỂễỄếẾệỆfFgGhHiIìÌỉỈĩĨíÍịỊjJkKlLmMnNoOòÒỏỎõÕóÓọỌôÔồỒổỔỗỖốỐộỘơƠờỜởỞỡỠớỚợỢpPqQrRsStTuUùÙủỦũŨúÚụỤưƯừỪửỬữỮứỨựỰvVwWxXyYỳỲỷỶỹỸýÝỵỴzZ0123456789!"#$%&''()*+,-./:;<=>?@[\]^_`{|}~ '
#vocab: 'aăâbcdđeêfghijklmnoôơpqrstuưvwxyzAĂÂBCDĐEÊFGHIJKLMNOÔƠPQRSTUƯVWXYZˋˊˀ˜﹒!"#$%&''()*+,-./=\@`~;?<>0123456789:'
# cpu, cuda, cuda:0
device: cuda:0
trainer:
  batch_size: 128
  print_every: 200
  valid_every: 4000
  iters: 100000
  # where to save our model for prediction
  export: /content/drive/MyDrive/out_text_recog/weights/transformerocr.pth
  checkpoint: /content/drive/MyDrive/out_text_recog/checkpoint/transformerocr_checkpoint.pth
  log: /content/drive/MyDrive/out_text_recog/train.log
  # null to disable compuate accuracy, or change to number of sample to enable validiation while training
  metrics: null

dataset:
  # name of your dataset
  name: ocr_data
  # path to annotation and image
  data_root: /content/synth_filtered
  train_annotation: train.txt
  valid_annotation: test.txt
  gen_data_path: /content/drive/MyDrive/out_text_recog
  # resize image to 32 height, larger height will increase accuracy
  image_height: 32
  image_min_width: 32
  image_max_width: 512

dataloader:
  num_workers: 3
  pin_memory: True

aug:
  image_aug: true
  masked_language_model: true

predictor:
  # disable or enable beamsearch while prediction, use beamsearch will be slower
  beamsearch: False

quiet: False

pretrain:
  id_or_url: 1nTKlEog9YFK74kPyX0qLwCWi60_YHHk4
  md5: efcabaa6d3adfca8e52bda2fd7d2ee04
  cached: /tmp/tranformerorc.pth

# url or local path
weights: https://drive.google.com/uc?id=1nTKlEog9YFK74kPyX0qLwCWi60_YHHk4

backbone: vgg19_bn
cnn:
  # pooling stride size
  ss:
    - [ 2, 2 ]
    - [ 2, 2 ]
    - [ 2, 1 ]
    - [ 2, 1 ]
    - [ 1, 1 ]
  # pooling kernel size
  ks:
    - [ 2, 2 ]
    - [ 2, 2 ]
    - [ 2, 1 ]
    - [ 2, 1 ]
    - [ 1, 1 ]
  # dim of ouput feature map
  hidden: 256
seq_modeling: seq2seq
transformer:
  encoder_hidden: 256
  decoder_hidden: 256
  img_channel: 256
  decoder_embedded: 256
  dropout: 0.1

optimizer:
  max_lr: 0.001
  pct_start: 0.1
